{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stallians/ML-Projects/blob/master/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it8OLVS2cJTC",
        "colab_type": "text"
      },
      "source": [
        "# IMDB Movie Review Sentiment Analysis\n",
        "\n",
        "**First version**:  \n",
        "~70% accuracy score on both train and test data. Looking for how to interpret the trained model so can find out the relevant models.  \n",
        "  1.1. Realised that weights for NB can't be determined straight-away. (Should've used a linear model instead)\n",
        "\n",
        "**Second version**:  \n",
        "Used Logistic Regression(LR), so that i can interpret the weights and improve feature vectors.  \n",
        "~73% accuracy on train and test data (0.74092, 0.7352) //suspected slight overfit\n",
        "\n",
        "**TODO**  \n",
        "1. Review Data can be cleaned throughly. Some HTML elements can be removed.\n",
        "2. Better values for Tfidf min and max df.\n",
        "3. There are more than one reviews for any given movie. Can this data point be used? Figure out. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3iye9-wnNNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1bfKTHRdPja",
        "colab_type": "text"
      },
      "source": [
        "## Get the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1gqZO1NL2Ik",
        "colab_type": "code",
        "outputId": "1389c9e0-b6a4-48ea-97b0-7d5a4ad61750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-20 18:12:48--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  22.9MB/s    in 3.9s    \n",
            "\n",
            "2019-12-20 18:12:52 (20.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2huEnJ_L36G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting the data\n",
        "import tarfile\n",
        "tar = tarfile.open('aclImdb_v1.tar.gz','r:gz')\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpVdjpeeu1B",
        "colab_type": "text"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3ydOwySOnGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_path_pos = 'aclImdb/test/pos/'\n",
        "test_path_neg = 'aclImdb/test/neg/'\n",
        "train_path_pos = 'aclImdb/train/pos/'\n",
        "train_path_neg = 'aclImdb/train/neg/'\n",
        "train_reviews, test_reviews = list(), list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7lUt755X6XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# method to read data into lists\n",
        "def read_reviews(filedir, sentiment):\n",
        "  temp_list = []\n",
        "  for tfilen in os.listdir(filedir):\n",
        "    filepath = os.path.join(filedir, tfilen)\n",
        "    rev = open(filepath).read()\n",
        "    temp_list.append([tfilen[:-4], rev, sentiment])\n",
        "  return temp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGal3ltaVMM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_reviews+=read_reviews(train_path_pos, 1)\n",
        "train_reviews+=read_reviews(train_path_neg, 0)\n",
        "\n",
        "test_reviews+=read_reviews(test_path_pos,1)\n",
        "test_reviews+=read_reviews(test_path_neg,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HByxqMsHXoSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use of assert is not favourable when code is optimised\n",
        "assert len(train_reviews) == 25000\n",
        "assert len(test_reviews) == 25000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ph2EV_0XriO",
        "colab_type": "code",
        "outputId": "f4425297-57d4-4240-bd54-05bfd44508d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "train_reviews[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['10152_9',\n",
              "  'I have looked forward to seeing this since I first saw it listed in her work. Finally found it yesterday 2/13/02 on Lifetime Movie Channel.<br /><br />Jim Larson\\'s comments about it being a \"sweet funny story of 2 people crossing paths\" were dead on. Writers probably shouldn\\'t get a bonus, everyone else SRO for making the movie.<br /><br />Anybody who appreciates a romantic Movie SHOULD SEE IT.<br /><br />Natasha\\'s screen presence is so warm and her smile so electric, to say nothing of her beauty, that anything she is in goes on my favorite list. Her TV and print interviews that I have seen are just as refreshing and well worth looking for.<br /><br />God Bless her, her family and future endeavors.<br /><br />This movie doesn\\'t seem to available in DVD or video yet, but I would be the first to buy it and I think others would too.',\n",
              "  1],\n",
              " ['3284_10',\n",
              "  'My name is John Mourby and this is my story about Paperhouse: In May 2003 I saw Alfred Hitchcock\\'s psycho, I was very scared and deeply disturbed. I began a frantic search for a film that was frightening in the same way. But none where satisfactory. Amongst those tried and failed were The Birds, Night of the Living Dead, The Silence of the Lambs, The Blair Witch Project, Ring, The Evil Dead, The Sixth Sense, 28 Days Later, The Texas Chainsaw Massacre, Halloween, Near Dark, Alien, Peeping Tom, The Cell, Rosemary\\'s Baby, Don\\'t Look Now, Witchfinder General, Friday the 13th and The Omen. That should confirm I was desperate! Long after I had stopped searching I found out about Paperhouse \\x85\\x85.<br /><br />Paperhouse is based on a favourite book that I own, called Marianne Dreams. Paperhouse had also come up in some of the books I had found on horror films, but they didn\\'t tell me about the link between book and film. I discovered the truth while on the Internet, I bought the film later that day.<br /><br />I thought Paperhouse would not be faithful to the book and dull. Unfaithal it certainly was but dull certainly not. It was the answer to my prayers Marianne is renamed Anna in the film but most of the original story is the same. One day in school Anna draws a house in her scrap book (nothing remarkable about that) then she becomes ill and every time she faints or falls asleep she finds herself outside a creepy old house (and I mean genuinely unnerving). More she also finds that every time she puts something new in the drawing it appears in the dreamworld, EG an apple tree. Anna draws into the dreamworld a rather sad boy named mark who apparently is a person in the real world. Mark is a cripple but wants to leave the house, obligingly Anna draws in a lighthouse (a place to go to) but still the problem remains mark can\\'t walk. So Anna decides to draw her father in. she gets her pencil out and gets too work, but the outcome is deformed and unsettling Anna particularly dislikes his eyes. Quote \"he looks like madman\". So Anna tries to rub him out and start again, but the pencil proves indelible (that means nothing can be rubbed out). Then Anna loses her temper and crosses out her father\\'s eyes! I leave you too find out for you self the terrible consequences of the rash action.<br /><br />Paperhouse truly is the British answer to A Nightmare on elm Street! The viewing of this film left me shocked and upset. But I have found what I was looking for after 2 years. The question is how dose the compare with Psycho? Answer, 1 the old dark house, 2 psychological parental fears, 3 a genuine shock, 4 and very scary music.',\n",
              "  1],\n",
              " ['6453_10',\n",
              "  \"The arrival of an world famous conductor sets of unexpected events and feelings in the small village. Some people are threatened by the way he handles the church choir, and how people in it gradually change. This movie is heartwarming and makes you leave the cinema with both a smile on your lips and tears in your eyes. It'a about bringing out the best in people and Kay Pollak has written an excellent script based on the ideas he has become so famous for. The actors are outstanding, Michael Nyqvist we know before but Frida Hallgren was an new, and charming acquaintances to me. She has a most vivid face that leaves no one untouched. Per Moberg does his part as Gabriella's husband almost too well, he is awful too see. One only wish the at he would be casted to play a nice guy one day so we can see if he masters that character as well.<br /><br />This is a movie that will not leave you untouched. If you haven't already seen it, do it today!\",\n",
              "  1],\n",
              " ['10743_9',\n",
              "  \"I've read most of the comments here. I came to the conclusion that almost everybody agrees that 9/11 is a shocking piece of history. There are a few who think that the added narrative is weak and <br /><br />I agree that the narrative is weak and unnecessary. About two brothers finding each other back after the disaster and the cliff hanger about Tony. But I don't think narration is unnecessary. Like I lot of theorists I think that our own lives are narrations. We are living and making our own autobiography. So if we tell about our lives this is always in the form of narration. We don't sum up facts like: Birth, Childhood, High School etc. We create a story about our live.<br /><br />Because we are familiar with stories, we want to put history in a story as well. Because in the form of a story we can identify ourselves. We can better understand the things happened in history when its told in the form of a story. So that's the purpose of adding a story in documentary. The story is weak, so be it, but we understand whats going on. If it was me out there I would be worried sick about my brother.<br /><br />And the second point, making a blockbuster movie about it. True, it's been to recent to come up with a big movie about 9/11. Though, there have been a few about the subject, but none of them like this documentary. But what if there will be a movie in about 5 years? I agree it is wrong trying to make a lot of money out of 9/11. But I also agree that movies are one of the best way to tell history. How many movies about the World war 2 have we seen? If I had not seen these movies my view of the WW would me totally different. I remember seeing Schindlers List, and I cried for an hour during class. Movies give you a good image of the things that happened in history and although it is fiction it contributes to the memory of the disasters and the casualties. <br /><br />So my point: telling stories is not always bad, it makes us identify with the story, and makes us never forget what happened.\",\n",
              "  1],\n",
              " ['2427_10',\n",
              "  'This is really a great unknown movie.Perfect dialogue without the typical clichés.This movie relied on the actor\\'s talent and it was pulled off.It even had a little bit a comedy in it,but it wasn\\'t overdone.Once in the Life is what a crime drama is supposed to be,not the typical special affects garbage with sex thrown in.I especially loved the interracial aspects of it all.<br /><br />Now onto the actors themselves. Laurence Fishburne was superb at playing a career petty criminal.It\\'s a shame that he\\'s only allowed to show his talent in his own movie. Titus Welliver was fabulous as Fishburne\\'s junky half-brother. Eamonn Walker added flavor to the already perfectly spiced film. Paul Calderon was perfect as a grease monkey/drug lord.I loved his acting since \"King of New York\". But the best acting in this film came from Gregory Hines and Michael Paul Chan,who were paired perfectly as two of Calderon\\'s henchmen.<br /><br />Once in the Life is for sure a keeper. ****1/2* out of *****.',\n",
              "  1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EINqqBmOY_ql",
        "colab_type": "code",
        "outputId": "25b53cb5-8079-48bf-f846-2bc537260167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "test_reviews[-5:]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['4622_1',\n",
              "  'This is the one movie that represents all that is bad in the movie business. The actors are pathetic and the script is awful. The special effects, if there are any, are so badly done that it would have been better to do it with cartoons instead. Besides that it\\'s great! I think the creators of the movie meant it to have humor, but the only time i was laughing was when I saw Patrick S. with long hair and the colorful costumes that every one had. The scenes at the end were good but they were not a part of the movie. In the end you will ask yourself \"why did I waste my time and money with that crap when I could have watched the plants growing or the clouds moving\". I don\\'t think that I am some critic or anything but this is a truly lame movie! DO NOT WATCH! DANGER OF STUPIDITY OVERLOAD!',\n",
              "  0],\n",
              " ['217_3',\n",
              "  \"I have to admit that I am disappointed after seeing this movie. I had expected so much more from the trailers. The movie was absolutely horrible. It lacked a real story line and the acting was not exactly the best. Don't waste your time. The movie is not what the trailers lead you to think it is. I would have to say that I don't usually write anything about movies on IMDb (in fact this is my first one) but this movie was such a disappointment that I registered just to let people know not to waste their time or money. The story line is that of a heist that is to happen and it looks like it had potential to be good but the things that happen in the movie are a little far fetched to be believable. Watch another movie instead, maybe the inside man???\",\n",
              "  0],\n",
              " ['6649_3',\n",
              "  \"Having broken into a secret database file for matching DNA serums,federal agent Frank Poo (Andy Garcia)discovers the only person who can save his son's life is a psychopath,played by Michael Keaton . However,when a serum transfer at the local hospital goes terribly wrong,a certain Mr.Poo has to do everything in his power to ensure the madman stays alive in order to make the inevitable transfer possible. By the way,his name is'nt really Poo,I just feel like calling it him. Despite the original concept at hand,this is an implausible and turgidly unexciting action thriller.I've never been a big fan of Andy Garcia,and granted his charecter here is'nt that attachable,this movie winds up all the worse.The action sequences are handled pretty disappointingly,and the ending sucks pretty bad. Having done a great villain in Pacific Heights,Keaton's psychopathic bad guy here is a let down,providing a madman too funny and charismatic to be deplored.Brian Cox is also wasted as Garcia's firm and frank superior.\",\n",
              "  0],\n",
              " ['9054_1',\n",
              "  'One IMDb reviewer calls Eaten Alive a passable film for the \"cannibal connoisseur.\" Are there such people? I didn\\'t know. But if you are one of them, hey, have a ball. The rest of you might find this tripe a bit hard to swallow (pun intended), even if, like me, you consider yourself a horror film connoisseur. I have been an avid horror fan for about 15 years now, although I never got around to the cannibal subgenre until a few weeks ago, and I guess I owe my short-lived interest in these groan-inducing movies, strangely enough, to China\\'s total disregard for copyright laws. You see, I bought a two-disc DVD collection of all of Wesley Snipes\\' films for 20 yuan (about $2.75), which turned out to include Last Cannibal World, Mountain of the Cannibal God, Eaten Alive, Cannibal Holocaust, Next, with Nicholas Cage, and something called Voodoo Lagoon, along with Blade 1-3, in Chinese. Nice.<br /><br />Being a second sequel, I immediately got a sinking feeling when the movie opened with a cannibal in street clothes wandering around major American cities, shooting unsuspecting Americans with poison darts and then scrambling away at full speed. Having run out of ways to keep movie cannibals scary, it seems that now they have made their way to the mainland. Later, you may be shocked to learn that this guy is on a \"training exercise.\" Lock up your daughters! <br /><br />Before long the movie settles into the old missing sister routine, as a young blonde woman named Sheila begins her own investigation of the disappearance of her sister, who looks nothing like her in any way, but she\\'s willing to spend most of the movie naked so I guess that doesn\\'t matter. It seems that, after shooting one of his victims, the hapless cannibal we met early in the movie, not used to big city life, ran into the road and was struck dead by a moving van. <br /><br />The brilliant police force find a mysterious bit of film on him showing Sheila\\'s sister involved in some bizarre ritual behavior, but other than the film, the guy is a complete mystery. As the piteous police chief laments, \"we know nothing about him except that he\\'s dead!\" Poor guy, he must be getting a headache from all this. I recommend a nap. Luckily, Sheila is the kind of girl who can throw around tens of thousands of dollars like it\\'s nothing in the search for her sister. Perfect for hiring a plucky backwoods guide caricature, since the police are clearly going to be no help.<br /><br />Obviously, nothing new is added to this miniscule sub-genre. Quite the contrary, cannibalism almost seems like a background to a completely different kind of bad movie, about the rescue of a missing person from the dangerous elements. Thickening the plot of that clothesline is not difficult, all you have to do is add in a cartoonish jungle cult of people who follow some guy who calls himself Jonas, who believes in using pain as a way to reunite man with nature, a process they call \"purification.\" Personally, I prefer just peeing outside occasionally.<br /><br />One of my favorite parts of the movie is when Sheila is caught by one of the cult members - an overweight guy who looks like he took a three-day weekend from the office to appear in this movie. As he pulls out his trusty medical kit to give her an injection, he warns her, \"If you don\\'t believe in Jonas and purification (through pain), God help you.\" He then gives her a shot and, when she winces from the tiny pinch, he politely apologizes to her. I sense a true believer in this guy!<br /><br />As far as the gore, there are plenty of nasty sound effects over random shots of animals getting slaughtered and more than enough disgusting footage of women being cut up and eaten alive, so I guess right there the movie lives up to its name. The acting is astonishingly bad, as can be expected, and interestingly enough, the editing is also spectacularly botched but still strangely effective. <br /><br />Unfortunately, I think you have to be able to relate to people who believe in utterly insane cults in order to relate to anyone in the movie. There are plenty of outlandish religious ceremonies that take place, which make it more and more difficult to understand why Sheila\\'s sister decided to turn her back on normal society. I\\'m all for individualism and doing your own thing, but come ON. <br /><br />After a while the movie descends even further into your basic, run of the mill escape movie, just before we witness the most wildly inappropriate rendition of Glory, Glory Hallelujah in film history. WOW. <br /><br />Note: in this movie, a woman is raped with a severed snake. If you need any more reason never to watch it, seek professional help. Avoid this mess at all costs.',\n",
              "  0],\n",
              " ['5441_1',\n",
              "  'After a string of successful \\'a man and his monkey films\\', which included the seminal \"Every Which Way But Loose\", \"Every Which Way You Can\" and \"Peter\\'s Friends\", the genre fell on hard times. In an effort to rejuvenate this once celebrated area, director Frank Marshall brought Michael Crichton\\'s acclaimed novel to the big screen.<br /><br />Think \\'Gorillas in the Mist\\' meets \\'Tron\\' minus the box-office clout of Bruce Boxleitner. This is one mans doomed love affair for his talking monkey. Not helped by bad accents (Tim Curry struggles with a Romanian), a baboon of a screenplay, hungry hippos, skydiving primates and Bruce Campbell. Ape-Sh*t.',\n",
              "  0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5IgnZVaathl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.DataFrame(data=train_reviews, columns=['filename','review', 'sentiment'])\n",
        "test = pd.DataFrame(data=test_reviews, columns=['filename','review', 'sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb-RLVqOFRxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, train_y = train.iloc[:,[0,1]], train.iloc[:,[2]]\n",
        "test_X, test_y = test.iloc[:,[0,1]], test.iloc[:,[2]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8LND7QscAZs",
        "colab_type": "code",
        "outputId": "7029321f-e5ab-4c38-b02f-9b8497d0df45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train_X.info()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25000 entries, 0 to 24999\n",
            "Data columns (total 2 columns):\n",
            "filename    25000 non-null object\n",
            "review      25000 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 390.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLzcLdbEbp8K",
        "colab_type": "code",
        "outputId": "c4aa71c4-10fb-45e9-919c-e7b5f78e3b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "test_X.info()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25000 entries, 0 to 24999\n",
            "Data columns (total 2 columns):\n",
            "filename    25000 non-null object\n",
            "review      25000 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 390.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHh68qMtIKGu",
        "colab_type": "text"
      },
      "source": [
        "## Transforming text into vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH1ouFRebw8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=0.2, max_df=0.8, ngram_range=(1,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLw6--2zJQq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = tfidf.fit_transform(train_X.review.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzmFk_LVdrht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_features = tfidf.transform(test_X.review.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6kkn8IlJZtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fdf = pd.DataFrame(features.todense(), columns=tfidf.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSmf4GEUd_e_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_fdf = pd.DataFrame(test_features.todense(), columns=tfidf.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLoxF_Cso10B",
        "colab_type": "code",
        "outputId": "b6805bc0-8706-49bc-841b-d5a7e4b4ba13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(fdf.shape)\n",
        "print(test_fdf.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 119)\n",
            "(25000, 119)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7myIyg-jo3Fo",
        "colab_type": "code",
        "outputId": "95f0247f-118c-4f0d-db25-e72d8692f532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "fdf.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>about</th>\n",
              "      <th>acting</th>\n",
              "      <th>after</th>\n",
              "      <th>all</th>\n",
              "      <th>also</th>\n",
              "      <th>an</th>\n",
              "      <th>and the</th>\n",
              "      <th>any</th>\n",
              "      <th>are</th>\n",
              "      <th>as</th>\n",
              "      <th>at</th>\n",
              "      <th>at the</th>\n",
              "      <th>bad</th>\n",
              "      <th>be</th>\n",
              "      <th>because</th>\n",
              "      <th>been</th>\n",
              "      <th>being</th>\n",
              "      <th>br</th>\n",
              "      <th>br br</th>\n",
              "      <th>br the</th>\n",
              "      <th>but</th>\n",
              "      <th>by</th>\n",
              "      <th>can</th>\n",
              "      <th>characters</th>\n",
              "      <th>could</th>\n",
              "      <th>do</th>\n",
              "      <th>don</th>\n",
              "      <th>even</th>\n",
              "      <th>film</th>\n",
              "      <th>first</th>\n",
              "      <th>for</th>\n",
              "      <th>for the</th>\n",
              "      <th>from</th>\n",
              "      <th>get</th>\n",
              "      <th>good</th>\n",
              "      <th>great</th>\n",
              "      <th>had</th>\n",
              "      <th>has</th>\n",
              "      <th>have</th>\n",
              "      <th>he</th>\n",
              "      <th>...</th>\n",
              "      <th>really</th>\n",
              "      <th>see</th>\n",
              "      <th>seen</th>\n",
              "      <th>she</th>\n",
              "      <th>so</th>\n",
              "      <th>some</th>\n",
              "      <th>story</th>\n",
              "      <th>than</th>\n",
              "      <th>the film</th>\n",
              "      <th>the movie</th>\n",
              "      <th>their</th>\n",
              "      <th>them</th>\n",
              "      <th>then</th>\n",
              "      <th>there</th>\n",
              "      <th>they</th>\n",
              "      <th>think</th>\n",
              "      <th>this film</th>\n",
              "      <th>this is</th>\n",
              "      <th>this movie</th>\n",
              "      <th>time</th>\n",
              "      <th>to be</th>\n",
              "      <th>to the</th>\n",
              "      <th>too</th>\n",
              "      <th>up</th>\n",
              "      <th>very</th>\n",
              "      <th>was</th>\n",
              "      <th>watch</th>\n",
              "      <th>way</th>\n",
              "      <th>we</th>\n",
              "      <th>well</th>\n",
              "      <th>were</th>\n",
              "      <th>what</th>\n",
              "      <th>when</th>\n",
              "      <th>which</th>\n",
              "      <th>who</th>\n",
              "      <th>will</th>\n",
              "      <th>with</th>\n",
              "      <th>with the</th>\n",
              "      <th>would</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.069793</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.059272</td>\n",
              "      <td>0.053753</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058643</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.09722</td>\n",
              "      <td>0.572792</td>\n",
              "      <td>0.286409</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.049667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.176238</td>\n",
              "      <td>0.099734</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086126</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117050</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079468</td>\n",
              "      <td>0.095079</td>\n",
              "      <td>0.093900</td>\n",
              "      <td>0.131304</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081736</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085061</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094309</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076121</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.092396</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.086137</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.066860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.158623</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.188229</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.124122</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.118545</td>\n",
              "      <td>0.043236</td>\n",
              "      <td>0.050116</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079079</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.231719</td>\n",
              "      <td>0.115865</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301386</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.098470</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.199465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100867</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166973</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039460</td>\n",
              "      <td>0.047374</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.379865</td>\n",
              "      <td>0.088530</td>\n",
              "      <td>0.049172</td>\n",
              "      <td>0.110219</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.112949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125249</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>0.056367</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.103347</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053382</td>\n",
              "      <td>0.124594</td>\n",
              "      <td>0.051853</td>\n",
              "      <td>0.103711</td>\n",
              "      <td>0.217013</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.060978</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058077</td>\n",
              "      <td>0.048418</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045080</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034218</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.053475</td>\n",
              "      <td>0.080926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.081259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.223979</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.138019</td>\n",
              "      <td>0.125167</td>\n",
              "      <td>0.072128</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.068277</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.133378</td>\n",
              "      <td>0.066692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057826</td>\n",
              "      <td>0.076437</td>\n",
              "      <td>0.085019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.101072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058059</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.249184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.409026</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.185046</td>\n",
              "      <td>0.110699</td>\n",
              "      <td>0.109326</td>\n",
              "      <td>0.152874</td>\n",
              "      <td>0.084911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097334</td>\n",
              "      <td>0.088627</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.215150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062457</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105298</td>\n",
              "      <td>0.211946</td>\n",
              "      <td>0.191315</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.103803</td>\n",
              "      <td>0.059088</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.092341</td>\n",
              "      <td>0.209615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.380086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050127</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047875</td>\n",
              "      <td>0.034922</td>\n",
              "      <td>0.121439</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.161395</td>\n",
              "      <td>0.029273</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.049703</td>\n",
              "      <td>0.095810</td>\n",
              "      <td>0.097088</td>\n",
              "      <td>0.094534</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.249549</td>\n",
              "      <td>0.124780</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079535</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095581</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027157</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044955</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063744</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.103558</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.143014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.311589</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050781</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154080</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083764</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029214</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.049253</td>\n",
              "      <td>0.495687</td>\n",
              "      <td>0.044744</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078216</td>\n",
              "      <td>0.041052</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036411</td>\n",
              "      <td>0.048554</td>\n",
              "      <td>0.082914</td>\n",
              "      <td>0.04932</td>\n",
              "      <td>0.086385</td>\n",
              "      <td>0.032682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.289908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094047</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.246428</td>\n",
              "      <td>0.094670</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.089616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.350126</td>\n",
              "      <td>0.175071</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.151798</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.119214</td>\n",
              "      <td>0.180834</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100346</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.135646</td>\n",
              "      <td>0.126147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.107372</td>\n",
              "      <td>...</td>\n",
              "      <td>0.124853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.130323</td>\n",
              "      <td>0.127754</td>\n",
              "      <td>0.116325</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.120475</td>\n",
              "      <td>0.120990</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.327905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.131631</td>\n",
              "      <td>0.109740</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102172</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077554</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 119 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      about    acting     after  ...  with the     would       you\n",
              "0  0.069793  0.000000  0.000000  ...   0.00000  0.158623  0.000000\n",
              "1  0.188229  0.000000  0.124122  ...   0.00000  0.053475  0.080926\n",
              "2  0.081259  0.000000  0.000000  ...   0.00000  0.092341  0.209615\n",
              "3  0.380086  0.000000  0.050127  ...   0.04932  0.086385  0.032682\n",
              "4  0.000000  0.289908  0.000000  ...   0.00000  0.000000  0.000000\n",
              "\n",
              "[5 rows x 119 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmRBhTOueufv",
        "colab_type": "code",
        "outputId": "5f97a818-bb66-41b3-a7a2-0a272cf20b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_y.sentiment.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GwDJq8Jed_D",
        "colab_type": "text"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EAbR8QTltRH",
        "colab_type": "text"
      },
      "source": [
        "### Using Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCGx4JaHecvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb = MultinomialNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgkCgJo9o-OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb=nb.fit(fdf, train_y.sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_oKMnpcyPE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions=nb.predict(fdf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4D5nCAYgU5b",
        "colab_type": "code",
        "outputId": "be309d92-8478-4e4b-9bcc-032ba4f5b77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# score on training data\n",
        "accuracy_score(train_y.sentiment, predictions)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZlfTKVEgV0u",
        "colab_type": "code",
        "outputId": "94d36d8c-0c01-409b-e29c-e6fcff6b2d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# score on testing data\n",
        "test_predictions = nb.predict(test_fdf)\n",
        "print(accuracy_score( test_y.sentiment, test_predictions))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.70752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hytoHl5xmnI6",
        "colab_type": "text"
      },
      "source": [
        "### Using linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjKixAC3msHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raEAa_1jm1xb",
        "colab_type": "code",
        "outputId": "de4ce54c-2790-4bff-f14e-033a43c4ab49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "lr = lr.fit(fdf, train_y.sentiment)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyL8ljALnDpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_predictions_lr = lr.predict(fdf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt3zYTNgnNbB",
        "colab_type": "code",
        "outputId": "a8d5fcd9-45d9-450c-971f-451171a9d999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# score on training data for  logistic(not 'linear') regression\n",
        "accuracy_score(train_y.sentiment, train_predictions_lr)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.74092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wegvSBBans4j",
        "colab_type": "code",
        "outputId": "785bf4f0-c400-4ae9-ade9-89b8049b7db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# score on testing data for logistic regression\n",
        "test_predictions_lr = lr.predict(test_fdf)\n",
        "print(accuracy_score(test_y.sentiment, test_predictions_lr))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iY3csHKhcJB",
        "colab_type": "text"
      },
      "source": [
        "## Interpretations from Linear Model\n",
        "- based on weights from logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXxILVgsa_z",
        "colab_type": "code",
        "outputId": "0d49e894-52e9-4dfb-8083-b1f682878644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(lr.coef_.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 119)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSI0vI9txSu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interpret = pd.DataFrame.from_dict({\"word\":tfidf.get_feature_names(),\n",
        "                                   \"weight\":lr.coef_[0].tolist() })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu4arApP0gDK",
        "colab_type": "code",
        "outputId": "c5f0dd4c-3aed-4afc-b57b-f444e0422a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "interpret.nlargest(5,'weight')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>great</td>\n",
              "      <td>5.752874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>well</td>\n",
              "      <td>3.297956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>also</td>\n",
              "      <td>2.542649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>one of</td>\n",
              "      <td>2.096197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>will</td>\n",
              "      <td>2.071102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       word    weight\n",
              "35    great  5.752874\n",
              "108    well  3.297956\n",
              "4      also  2.542649\n",
              "72   one of  2.096197\n",
              "114    will  2.071102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srv2K32Q1B2T",
        "colab_type": "code",
        "outputId": "9582c888-9c28-46ab-e7b4-5636d2f2d85b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "interpret.nsmallest(5,'weight')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>bad</td>\n",
              "      <td>-7.402652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>no</td>\n",
              "      <td>-3.590220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>even</td>\n",
              "      <td>-2.902132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>acting</td>\n",
              "      <td>-2.747727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>plot</td>\n",
              "      <td>-2.742868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word    weight\n",
              "12     bad -7.402652\n",
              "66      no -3.590220\n",
              "27    even -2.902132\n",
              "1   acting -2.747727\n",
              "78    plot -2.742868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM-vykdZiZkB",
        "colab_type": "text"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rttb4q7haDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "First run:\n",
        "~70% accuracy score on both train and test data. Looking for how to interpret the trained model so can find out the relevant models.\n",
        "  * realised that weights for NB can't be determined straight-away. (Should've used a linear model instead)\n",
        "\n",
        "Second version:\n",
        "tried to use LR(linear) (linear model), so that i can interpret the weights and improve feature vectors\n",
        "LR worked but, could not remember how to use it for binary classification ???\n",
        "\n",
        "Seriously??? I should have used logistic model instead.\n",
        "++ changed the model to logistic regression\n",
        "  Second run:\n",
        "  ~ 73% accuracy on train and test data (0.74092, 0.7352) //suspected slight overfitting\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WiQQ_zwtuvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Tips:\n",
        "1. to get the values of various hyperparameters/coefficient/weights of trained models/transformers/vectorizers, \n",
        "look into \"Attributes\" section of the method's scikit learn documentation\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}